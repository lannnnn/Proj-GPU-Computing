The self developed project for course GPU-Computing
The developing will be split in several steps:

1. finish and full test of the usage of CSR format       passed
2. test the parMetis method as the current baseline - problem: Metis use the different input format(convert); the usage of Metis is much different with our design - the partition number should be fixed at very first and it is using MPI instead of GPU. Temperarily quit this part.
3. develope the sequential code as our first version     passed
4. optimizing... specific methods should be decleared later according to the performance

Usage: 
    for CPU code: in this dir run `make all`
        the target named 'blocktest' will be build in this dir
    for CUDA code: make sure have nvcc compiler. run `make cuda`
        the target named 'block_cuda' and 'ref_block_cuda'

The block_cuda use the alg as:
    We treat each group as row, so when two rows is near enough, we produce a new row and disable the original rows. Each thread will take  responsibility of some of the rows. Due to the reason that the size of label(cols message of each row), row number and group size all varies, it's not easy to realize this alg in GPU, mostly due to we will never be sure the final size of group numbers. It's never going to be a proper reduction... So when we have meet large sparse matrix with very low density, the performance will be extremely terrible.... Not make sense to parallelize
    ***NOT PROPERLY WORK SINCE WILL BE REALLY SLOW, SO NOT ADAPTING TO MULTI-BLOCKS***

The ref_block_cuda use the alg as:
    We first choose some of the reference rows(in our case, 4 rows, defined in the cuda_impl.h) that it's far enough to each other. And each thread calculate the distance of the row they are responsible for with these reference rows. If the distance is all larger than tau, then do nothing; but if one of then smaller than tau, add the row into the group which has smallest distance. This algorithm was benefit from parallelize, however not make use of the updated group property since updating the group message is not developed, otherwise huge overhead should be involved due to synchronization issue... 

To run the code in cluster, referring to the script 'ref_test.sh'. This script can be directly use in MARZOLA

------------------------------------------------------------------------------------------------------------------

Updated 24/06 - QUIT
If do not make usage of the block boundary message, the result of reordering may not give benefits for different block size.
Example: we have
 line1  1 0 0 0 1 0 0 0
 line2  0 0 0 0 1 0 0 0
 line3  0 0 0 0 0 1 1 1
When considering the block size = (4,4), according to Jaccard distance, line2 will be groupped with line1. But obviously, it will perform worse, but should be groupped with line3.
Plan to make usage of the block boundary message.... 
Idea: instead of using a whole line message to calculate the Jaccard distance, using a "mask" which will present the block boundary message

Also, if a group do not have the # rows which can be devide by block col, it means there be blocks combined by two different groups. It's not good for reordering...
Idea: filling the group with the empty rows so no block will be combine by different groups... 
------------------------------------------------------------------------------------------------------------------

DATASET STATUS:(cluster queue time limit to 5 minutes)

1. the complexity is bring from [1] O(1) find the refering row(s) [2] comparing with other rows, each comparation will have at largest 2*min(rank1,rank2) times comparing, so at worst case will have the complexity of nrows*(nrows-1)*nnz
2. tau and ref_thread_size(in fact another alg) will effect the execution time - the larger tau/ref_thread_size, the longger time spend
2*. decreasing tau do not really means the new density will increasing - but with the idea in Updated 24/06, the density should increase in theory..
3. memory boundary is for the size of CSR matrix, not yet met. The boundary (global memory size of CPU/GPU(A30)) is for((nrows+1) + ncols) * sizeof(int)[CSR] + (nrows + ref_thread_size) * sizeof(int)[Msg use for calculation] + sizeof(float)[tau]

DATASIZE(nrows ncols nnz)   CPU time    GPU time    density(ori->new)
tau = 0.6 block size = 8*8
(28, 29, 250)               0.04ms      0.54784ms   0.390625->0.325521
(60, 61, 353)               0.259ms     2.50163ms   0.16714->0.141426
(289, 289, 1089)            4.379ms     26.3342ms   0.122415->0.0802624
(494, 494, 1080)            6.43ms      50.6829ms   0.0428299->0.0349379
(1138, 1138, 2596)          40.063ms    221.846ms   0.0561807->0.043804
(2358, 2358, 9815)          211.375ms   533.595ms   0.126848->0.102582
(3296, 3296, 6432)          1161.43ms   1678.89ms   0.0492165->0.0462494
(3918, 3918, 16697)         1094.61ms   1574.49ms   0.122888->0.104733
(6144, 6143, 613871)        12745.5ms   5195.88ms   0.205136->0.135628
(8141, 8141, 1012521)       41882ms     2207.37ms   0.599766->0.625693
(8275, 8298, 103689)        19985.7ms   9087.07ms   0.022288->0.0248805
(9507, 9507, 591626)        15785.2ms   1316.35ms   0.706416->0.688475
(9507, 9507, 684169)        14990.2ms   1156.89ms   0.715251->0.703067
(11949, 11949, 80519)       168349ms    18287.8ms   0.113702->0.0911145
(12009, 12009, 118522)      51114.3ms   17404.9ms   0.0182708->0.0349614
(21608, 94757, 238714)      TLE         164399ms    0.0336473->0.0336865
(58226, 58228, 214078)      TLE         160861ms    0.0259844->0.026782
(65536, 65536, 2456398)     TLE         TLE

tau = 0.4 block size = 8*8
(28, 29, 250)               0.088ms     1.12435 ms  0.390625->0.390625
(60, 61, 353)               0.472ms     4.20557ms   0.16714->0.157589
(289, 289, 1089)            7.851ms     44.4692ms   0.122415->0.10189
(494, 494, 1080)            13.563ms    74.0106ms   0.0428299->0.0387041
(1138, 1138, 2596)          151.672ms   290.296ms   0.0561807->0.0470563
(2358, 2358, 9815)          2971.23ms   1589.28ms   0.126848->0.10163
(3296, 3296, 6432)          1162.94ms   1735.68ms   0.0492165->0.0462494
(3918, 3918, 16697)         15662.7ms   4534.65ms   0.122888->0.104231
(6144, 6143, 613871)        48540.3ms   14078.9ms   0.205136->0.214221
(8141, 8141, 1012521)       129475ms    4094.99ms   0.599766->0.635546
(8275, 8298, 103689)        59768.4ms   12475.4ms   0.022288->0.0232485
(9507, 9507, 591626)        22881.4ms   1905.69ms   0.706416->0.695468
(9507, 9507, 684169)        21789.9ms   1743.23ms   0.715251->0.696789
(11949, 11949, 80519)       TLE         30855.5ms   0.113702->0.101912
(12009, 12009, 118522)      151599ms    27288.6ms   0.0182708->0.0307779
(21608, 94757, 238714)      TLE         223140ms    0.0336473->0.033627
(58226, 58228, 214078)      TLE         TLE
(65536, 65536, 2456398)     TLE         TLE
