The self developed project for course GPU-Computing
The developing will be split in several steps:

1. finish and full test of the usage of CSR format       passed
2. test the parMetis method as the current baseline - problem: Metis use the different input format(convert); the usage of Metis is much different with our design - the partition number should be fixed at very first and it is using MPI instead of GPU. Temperarily quit this part.
3. develope the sequential code as our first version     passed
4. optimizing... specific methods should be decleared later according to the performance

Usage: 
    for CPU code: in this dir run `make all`
        the target named 'blocktest' will be build in this dir
    for CUDA code: make sure have nvcc compiler. run `make cuda`
        the target named 'block_cuda' and 'ref_block_cuda'

The block_cuda use the alg as:
    We treat each group as row, so when two rows is near enough, we produce a new row and disable the original rows. Each thread will take  responsibility of some of the rows. Due to the reason that the size of label(cols message of each row), row number and group size all varies, it's not easy to realize this alg in GPU, mostly due to we will never be sure the final size of group numbers. It's never going to be a proper reduction... So when we have meet large sparse matrix with very low density, the performance will be extremely terrible.... Not make sense to parallelize

The ref_block_cuda use the alg as:
    We first choose some of the reference rows(in our case, 4 rows, defined in the cuda_impl.h) that it's far enough to each other. And each thread calculate the distance of the row they are responsible for with these reference rows. If the distance is all larger than tau, then do nothing; but if one of then smaller than tau, add the row into the group which has smallest distance. This algorithm was benefit from parallelize, however not make use of the updated group property since updating the group message is not developed, otherwise huge overhead should be involved due to synchronization issue... 

To run the code in cluster, referring to the script 'ref_test.sh'. This script can be directly use in MARZOLA

------------------------------------------------------------------------------------------------------------------

Updated 24/06 
If do not make usage of the block boundary message, the result of reordering may not give benefits for different block size.
Example: we have
 line1  1 0 0 0 1 0 0 0
 line2  0 0 0 0 1 0 0 0
 line3  0 0 0 0 0 1 1 1
When considering the block size = (4,4), according to Jaccard distance, line2 will be groupped with line1. But obviously, it will perform worse, but should be groupped with line3.
Plan to make usage of the block boundary message.... 
Idea: instead of using a whole line message to calculate the Jaccard distance, using a "mask" which will present the block boundary message
